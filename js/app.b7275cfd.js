(function(e){function t(t){for(var i,n,o=t[0],l=t[1],c=t[2],d=0,h=[];d<o.length;d++)n=o[d],Object.prototype.hasOwnProperty.call(r,n)&&r[n]&&h.push(r[n][0]),r[n]=0;for(i in l)Object.prototype.hasOwnProperty.call(l,i)&&(e[i]=l[i]);p&&p(t);while(h.length)h.shift()();return s.push.apply(s,c||[]),a()}function a(){for(var e,t=0;t<s.length;t++){for(var a=s[t],i=!0,n=1;n<a.length;n++){var o=a[n];0!==r[o]&&(i=!1)}i&&(s.splice(t--,1),e=l(l.s=a[0]))}return e}var i={},n={app:0},r={app:0},s=[];function o(e){return l.p+"js/"+({classroom:"classroom",home:"home",progress:"progress",projects:"projects",resources:"resources"}[e]||e)+"."+{classroom:"233ea7a9",home:"ae0cfaa9",progress:"cff978ae",projects:"4db3f096",resources:"40bd39b6"}[e]+".js"}function l(t){if(i[t])return i[t].exports;var a=i[t]={i:t,l:!1,exports:{}};return e[t].call(a.exports,a,a.exports,l),a.l=!0,a.exports}l.e=function(e){var t=[],a={home:1,progress:1,projects:1};n[e]?t.push(n[e]):0!==n[e]&&a[e]&&t.push(n[e]=new Promise((function(t,a){for(var i="css/"+({classroom:"classroom",home:"home",progress:"progress",projects:"projects",resources:"resources"}[e]||e)+"."+{classroom:"31d6cfe0",home:"c5b53d78",progress:"cb82569e",projects:"7a2da886",resources:"31d6cfe0"}[e]+".css",r=l.p+i,s=document.getElementsByTagName("link"),o=0;o<s.length;o++){var c=s[o],d=c.getAttribute("data-href")||c.getAttribute("href");if("stylesheet"===c.rel&&(d===i||d===r))return t()}var h=document.getElementsByTagName("style");for(o=0;o<h.length;o++){c=h[o],d=c.getAttribute("data-href");if(d===i||d===r)return t()}var p=document.createElement("link");p.rel="stylesheet",p.type="text/css",p.onload=t,p.onerror=function(t){var i=t&&t.target&&t.target.src||r,s=new Error("Loading CSS chunk "+e+" failed.\n("+i+")");s.code="CSS_CHUNK_LOAD_FAILED",s.request=i,delete n[e],p.parentNode.removeChild(p),a(s)},p.href=r;var u=document.getElementsByTagName("head")[0];u.appendChild(p)})).then((function(){n[e]=0})));var i=r[e];if(0!==i)if(i)t.push(i[2]);else{var s=new Promise((function(t,a){i=r[e]=[t,a]}));t.push(i[2]=s);var c,d=document.createElement("script");d.charset="utf-8",d.timeout=120,l.nc&&d.setAttribute("nonce",l.nc),d.src=o(e);var h=new Error;c=function(t){d.onerror=d.onload=null,clearTimeout(p);var a=r[e];if(0!==a){if(a){var i=t&&("load"===t.type?"missing":t.type),n=t&&t.target&&t.target.src;h.message="Loading chunk "+e+" failed.\n("+i+": "+n+")",h.name="ChunkLoadError",h.type=i,h.request=n,a[1](h)}r[e]=void 0}};var p=setTimeout((function(){c({type:"timeout",target:d})}),12e4);d.onerror=d.onload=c,document.head.appendChild(d)}return Promise.all(t)},l.m=e,l.c=i,l.d=function(e,t,a){l.o(e,t)||Object.defineProperty(e,t,{enumerable:!0,get:a})},l.r=function(e){"undefined"!==typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(e,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(e,"__esModule",{value:!0})},l.t=function(e,t){if(1&t&&(e=l(e)),8&t)return e;if(4&t&&"object"===typeof e&&e&&e.__esModule)return e;var a=Object.create(null);if(l.r(a),Object.defineProperty(a,"default",{enumerable:!0,value:e}),2&t&&"string"!=typeof e)for(var i in e)l.d(a,i,function(t){return e[t]}.bind(null,i));return a},l.n=function(e){var t=e&&e.__esModule?function(){return e["default"]}:function(){return e};return l.d(t,"a",t),t},l.o=function(e,t){return Object.prototype.hasOwnProperty.call(e,t)},l.p="/",l.oe=function(e){throw console.error(e),e};var c=window["webpackJsonp"]=window["webpackJsonp"]||[],d=c.push.bind(c);c.push=t,c=c.slice();for(var h=0;h<c.length;h++)t(c[h]);var p=d;s.push([0,"chunk-vendors"]),a()})({0:function(e,t,a){e.exports=a("56d7")},"034f":function(e,t,a){"use strict";var i=a("64a9"),n=a.n(i);n.a},"0f23":function(e,t,a){"use strict";a.r(t);var i=function(){var e=this,t=e.$createElement;e._self._c;return e._m(0)},n=[function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("section",[a("h2",[e._v("Introduction")]),a("p",[e._v("Traditionally various tasks, such as discovering market trends and predicting future prices of assets have been addressed with charts and line graphs in the financial data domain. Analyzing E-transaction time-series in a temporal context is critical for understanding transaction behavior, learning user preferences, and discovering temporal trends. This point of sale data is temporal, multivariate, and spatial in nature; therefore, it is well suited for analysis in a visual analytics environment. However, it is difficult to find systems that manage the characteristics of point of sale data effectively.")]),a("p",[a("strong",[e._v("MarketAnalyzer")]),e._v(" leveragesan enhanced pixel-based visualization approach to efficiently utilize limited screen space for the large store and product information. It also allows exploring current sales volume, trend, and temporal market share growth rates using a series of linked views. Another important visual tool for E-transactions time-series is introduced in "),a("strong",[e._v("VAET")]),e._v(".")]),a("h2",[e._v("Project Task")]),a("ul",[a("li",[a("p",[e._v("Reproduce visualizations in MarketAnalyzer "),a("strong",[e._v("or")]),e._v(" VAET for business data in d3")])]),a("li",[a("p",[e._v("Analyze novel patterns in the data")])]),a("li",[a("p",[e._v("Find out why author use this visualization method")])]),a("li",[a("p",[e._v("Summarize shortcomings and consider how to improve.")])])]),a("h2",[e._v("Schedule")]),a("ul",[a("li",[a("p",[e._v("Reproduce one paper result (components) using d3 or other visualization tool (3-7 week)")])]),a("li",[a("p",[e._v("Mid-term inspection (8 week)")])]),a("li",[a("p",[e._v("Find some patterns in a dataset (9-10 week)")])]),a("li",[a("p",[e._v("Find a different way to visualize the dataset (11-13 week)")])]),a("li",[a("p",[e._v("Compare your visualization method with paper’s (14-15 week)")])]),a("li",[a("p",[e._v("Final inspection (16-17 week)")])])]),a("h2",[e._v("Implementation tools：")]),a("ul",[a("li",[e._v("D3")])]),a("h2",[e._v("Resources：")]),a("ul",[a("li",[a("p",[e._v("Papers:  https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/j.1467-8659.2012.03117.x\nhttps://www.researchgate.net/publication/264273467")])]),a("li",[a("p",[e._v("Datasets:")]),a("ul",[a("li",[e._v("MarketAnalyzer Data: https://data.world/garyhoov/2012-us-retail-sales")]),a("li",[e._v("VAET Data: https://www.kaggle.com/ntnu-testimon/paysim1")])])])])])}],r={components:{}},s=r,o=a("2877"),l=Object(o["a"])(s,i,n,!1,null,null,null);t["default"]=l.exports},3504:function(e){e.exports=JSON.parse('{"bigdataTitle":"神经网络与深度学习","homePage":"课程首页","resultList":"历史资源","courseEntriesHint":"（仅限选课学生进入）","location":"山东大学・青岛","courseIntro":"课程介绍","courseIntroContent":"神经网络与深度学习作为人工智能课程体系中的一门基础课程，已被纳入我国开设人工智能专业的高校的培养计划。该课程是为我校人工智能和大数据专业的本科生开设的，旨在为学生打下坚实的数学基础和人工智能理论知识，培养学生的实验开发和科学研究能力。","courseDesc":"课程描述","courseDescContent":"这门神经网络和深度学习课程广泛介绍了深度学习的基础，如何构建神经网络和如何领导成功的机器学习项目等实用技能，以及卷积网络、RNN、LSTM等基本模型。本课程还将提供来自医疗保健、自动驾驶和自然语言处理等方面的案例研究。学生需要掌握理论，并通过在真实世界的数据中应用方法来实践这些想法。","aims":"教学目标","aims1":"本课程以任务为导向，通过实例讲授程序设计的基本概念和方法，培养学生良好的编程习惯，了解系统开发的工程规范。","aims2":"强调以动手实践计算机编程为出发点，注重设计思路，即问题分析、模型构建、算法设计和程序实现。","aims3":"培养处理实际问题的能力，能根据问题的具体需要实现数据的采样、存储和管理，并给出相应的算法设计。","aims4":"能够选择合适的设计工具，运用适当的相关技术手段，提高解决复杂工程问题的能力和效率。","request":"教学要求","requestContent":"通过本课程的学习，主要培养学生以下能力：","requestContent1":"工程知识。培养学生建立神经网络和深度学习的基本概念，了解主流神经网络的架构。","requestContent2":"问题分析。帮助学生培养分析问题和解决问题的思维，培养学生对系统或过程进行抽象分析和识别，选择或建立模型抽象表达，并进行推理、求解和验证的能力。","requestContent3":"设计/开发解决方案。培养学生掌握神经网络的使用，有效设计、训练和应用深度学习技术，使学生具备深度学习算法设计、分析和优化的能力。","reference":"教材及参考资料","authors":"编著者","pub":"出版社","pubYear":"出版年","ptPress":"人民邮电出版社","phei":"电子工业出版社","noResult":"暂无成果信息","fetching":"正在获取文件列表…","fileName":"文件名","url":"地址","teacherAndTA":"授课教师与助教信息","LOT":"教学大纲","week":"周次","date":"日期","topic":"主题","grading":"评分细则","exp":"实验","paperreading":"文献阅读与展示","finalexam":"期末考试","reslink":"链接"}')},"38a8":function(e,t,a){"use strict";a.r(t);var i=function(){var e=this,t=e.$createElement;e._self._c;return e._m(0)},n=[function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("section",[a("h2",[e._v("Challenge Descriptions")]),a("p",[e._v("After the successful resolution of the 2014 kidnapping at GAStech’s Abila, Kronos office, GAStech officials determined that Abila offices needed a significant upgrade. In 2015, the growing company moved into a state-of-the-art three-story building near their previous location. The new office is built to the highest energy efficiency standards, and it is fully instrumented with sensors that identify everything from building temperatures to concentration levels of various chemicals.")]),a("p",[e._v("GAStech has recently introduced new security processes. Staff members are now required to wear proximity (prox) cards while in the building, so that incidents like the 2014 kidnapping cannot occur again.")]),a("p",[e._v("As an expert in visual analytics, you have been hired to help GAStech understand its steady stream of operations data. This includes data from stationary and mobile sensors of multiple types. The company needs your help in operational issues as well as security issues. Can you identify the issues to safeguard the company’s employees?")]),a("p",[e._v("The new office is built to the highest energy efficiency standard, but as with any new building, there are still several HVAC issues to work out. The building is divided into several HVAC (heating, ventilation, and air conditioning) zones. Each zone is instrumented with sensors that report building temperatures, heating and cooling system status values, and concentration levels of various chemicals such as carbon dioxide (abbreviated CO2) and hazium (abbreviated Haz), a recently discovered and possibly dangerous chemical. CEO Sten Sanjorge Jr. has read about hazium and requested that these sensors be included. However, they are very new and very expensive, so GAStech can afford only a small number of sensors.")]),a("p",[e._v("With their move into the new building, GAStech also introduced new security procedures, which staff members are not necessarily adopting consistently. Staff members are now required to wear proximity (prox) cards while in the building.")]),a("p",[e._v("The building is instrumented with passive prox card readers that cover individual building zones. The prox card zones do not generally correspond with the HVAC zones. When a prox card passes into a new zone, it is detected and recorded. Most, but not all, areas are still open to staff members even if they forget their prox cards. People are somewhat careless with their prox cards, but some diligent staff members will go to the security desk and pick up a new prox card if their old one is mislaid.")]),a("p",[e._v("As part of the deal to entice GAStech to move into this new building, the builders included a free robotic mail delivery system. This robot, nicknamed Rosie, travels the halls periodically, moving between floors in a specially designed chute. Rosie is equipped with a mobile prox sensor, which identifies the prox cards in the areas she travels through.")]),a("p",[e._v("As an expert in visual analytics, you have been hired to help GAStech understand its operations data. In this challenge, you are given two weeks of building and prox sensor data. Can you use visual analytics to identify typical patterns of and issues of concern? You will have the following data and supporting information at your disposal:")]),a("ul",[a("li",[e._v("A building layout for the GAStech offices, including the maps of the prox zones and the HVAC zones")]),a("li",[e._v("A current list of employees, roles, and office assignments")]),a("li",[e._v("A description of the data formats and fields provided")]),a("li",[e._v("Proximity sensor data for each of the prox zone regions")]),a("li",[e._v("Proximity sensor data from Rosie the mobile robot")]),a("li",[e._v("HVAC sensor readings and status information from each of the building’s HVAC zones")]),a("li",[e._v("Hazium readings from four sensors.")])]),a("h2",[e._v("Project Task")]),a("ul",[a("li",[a("p",[e._v("What are the typical patterns in the prox card data? What does a typical day look like for GAStech employees?")])]),a("li",[a("p",[e._v("Describe up to ten of the most interesting patterns that appear in the building data. Describe what is notable about the pattern and explain its possible significance.")])]),a("li",[a("p",[e._v("Describe up to ten notable anomalies or unusual events you see in the data. Prioritize those issues that are most likely to represent a danger or a serious issue for building operations.")])]),a("li",[a("p",[e._v("Describe up to five observed relationships between the proximity card data and building data elements. If you find a causal relationship (for example, a building event or condition leading to personnel behavior changes or personnel activity leading to building operations changes), describe your discovered cause and effect, the evidence you found to support it, and your level of confidence in your assessment of the relationship.")])])]),a("h2",[e._v("Schedule")]),a("ul",[a("li",[a("p",[e._v("Fetch Dataset, analyse tasks, and make a plan to solve it (2-3 week)")])]),a("li",[a("p",[e._v("Process data, implement analysis framework (4-7 week)")])]),a("li",[a("p",[e._v("Mid-term inspection (8 week)")])]),a("li",[a("p",[e._v("Implement analysis system (9-12 week)")])]),a("li",[a("p",[e._v("Inter-team evaluation and modification (13-15 week)")])]),a("li",[a("p",[e._v("Final inspection (16-17 week)")])])]),a("h2",[e._v("Resources")]),a("ul",[a("li",[e._v("http://www.vacommunity.org/2016+VAST+Challenge%3A+MC2")])])])}],r={components:{}},s=r,o=a("2877"),l=Object(o["a"])(s,i,n,!1,null,null,null);t["default"]=l.exports},"46c4":function(e,t,a){"use strict";a.r(t);var i=function(){var e=this,t=e.$createElement;e._self._c;return e._m(0)},n=[function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("section",[a("h2",[e._v("Challenge Descriptions")]),a("p",[e._v("St. Himark is a vibrant community located in the Oceanus Sea. Home to the world-renowned St. Himark Museum, beautiful beaches, and the Wilson Forest Nature Preserve, St. Himark is one of the region’s best cities for raising a family and provides employment across a number of industries including the Always Safe Nuclear Power Plant. Well, all that was true before the disastrous earthquake that hits the area during the course of this year’s challenge. Mayor Jordan, city officials, and emergency services are overwhelmed and are desperate for assistance in understanding the true situation on the ground and how best to deploy the limited resources available to this relatively small community.")]),a("p",[e._v("One of St. Himark’s largest employers is the Always Safe nuclear power plant. The pride of the city, it produces power for St. Himark’s needs and exports the excess to the mainland providing a steady revenue stream. However, the plant was not compliant with international standards when it was constructed and is now aging. As part of its outreach to the broader community, Always Safe agreed to provide funding for a set of carefully calibrated professional radiation monitors at fixed locations throughout the city. Additionally, a group of citizen scientists led by the members of the Himark Science Society started an education initiative to build and deploy lower cost homemade sensors, which people can attach to their cars. The sensors upload data to the web by connecting through the user’s cell phone. The goal of the project was to engage the community and demonstrate that the nuclear plant’s operations were not significantly changing the region’s natural background levels of radiation.")]),a("p",[e._v("When an earthquake strikes St. Himark, the nuclear power plant suffers damage resulting in a leak of radioactive contamination. Further, a coolant leak sprayed employees’ cars and contaminated them at varying levels. Now, the city’s government and emergency management officials are trying to understand if there is a risk to the public while also responding to other emerging crises related to the earthquake as well as satisfying the public’s concern over radiation.")]),a("h2",[e._v("Project Task")]),a("p",[e._v("Your task, as supported by visual analytics that you apply, is to help St. Himark’s emergency management team combine data from the government-operated stationary monitors with data from citizen-operated mobile sensors to help them better understand conditions in the city and identify likely locations that will require further monitoring, cleanup, or even evacuation. Will data from citizen scientists clarify the situation or make it more uncertain? Use visual analytics to develop responses to the questions below. Novel visualizations of uncertainty are especially interesting for this mini-challenge.")]),a("ol",[a("li",[e._v("Visualize radiation measurements over time from both static and mobile sensors to identify areas where radiation over background is detected. Characterize changes over time.")]),a("li",[e._v("Use visual analytics to represent and analyze uncertainty in the measurement of radiation across the city.\n"),a("ol",[a("li",[e._v("Compare uncertainty of the static sensors to the mobile sensors. What anomalies can you see? Are there sensors that are too uncertain to trust?")]),a("li",[e._v("Which regions of the city have greater uncertainty of radiation measurement? Use visual analytics to explain your rationale.")]),a("li",[e._v("What effects do you see in the sensor readings after the earthquake and other major events? What effect do these events have on uncertainty?")])])]),a("li",[e._v("Given the uncertainty you observed in question 2, are the radiation measurements reliable enough to locate areas of concern?\n"),a("ol",[a("li",[e._v("Highlight potential locations of contamination, including the locations of contaminated cars. Should St. Himark officials be worried about contaminated cars moving around the city?")]),a("li",[e._v("Estimate how many cars may have been contaminated when coolant leaked from the Always Safe plant. Use visual analysis of radiation measurements to determine if any have left the area.")]),a("li",[e._v("Indicated where you would deploy more sensors to improve radiation monitoring in the city. Would you recommend more static sensors or more mobile sensors or both? Use your visualization of radiation measurement uncertainty to justify your recommendation.")])])]),a("li",[e._v("Summarize the state of radiation measurements at the end of the available period. Use your novel visualizations and analysis approaches to suggest a course of action for the city. Use visual analytics to compare the static sensor network to the mobile sensor network. What are the strengths and weaknesses of each approach? How do they support each other?")])]),a("h2",[e._v("Schedule")]),a("ul",[a("li",[a("p",[e._v("Fetch Dataset, analyse tasks, and make a plan to solve it (2-3 week)")])]),a("li",[a("p",[e._v("Process data, implement analysis framework (4-7 week)")])]),a("li",[a("p",[e._v("Mid-term inspection (8 week)")])]),a("li",[a("p",[e._v("Implement analysis system (9-12 week)")])]),a("li",[a("p",[e._v("Inter-team evaluation and modification (13-15 week)")])]),a("li",[a("p",[e._v("Final inspection (16-17 week)")])])]),a("h2",[e._v("Resources")]),a("ul",[a("li",[e._v("https://vast-challenge.github.io/2019/MC2.html")])])])}],r={components:{}},s=r,o=a("2877"),l=Object(o["a"])(s,i,n,!1,null,null,null);t["default"]=l.exports},5072:function(e,t,a){"use strict";a.r(t);var i=function(){var e=this,t=e.$createElement;e._self._c;return e._m(0)},n=[function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("section",[a("h2",[e._v("Introduction")]),a("p",[e._v("人体姿态估计是一个应用十分广泛的课题。它的基本任务是：将图片中人体的重要关节点标注出来，并按照人体结构进行拓扑连接。人体姿态估计技术在体育健身、动作采集、3D试衣、舆情监测等领域具有广阔的应用前景。OpenPose是基于卷积神经网络和监督学习，并以caffe为框架写成的开源库，可以实现人的面部表情、躯干和四肢甚至手指的跟踪，同时具有较好的鲁棒性，可以称作世界上第一个基于深度学习的实时多人二维姿态估计。")]),a("p",[e._v("该项目的主要思想：1、对输入的图像进行卷积，提取相关特征；2、对提取的特征分别获取Part Confidence Maps和Part Affinity Fields，获取到图像中的各个肢体的所在位置；3、根据上一步的信息得到Part Association，匹配起同一个人的关节点并连接，形成整体骨架。")]),a("h2",[e._v("Project Task")]),a("ul",[a("li",[a("p",[e._v("运行OpenPose项目代码，使用不同的图片数据集进行测试")])]),a("li",[a("p",[e._v("记录标记不成功的例子，提出改进思路并实践")])])]),a("h2",[e._v("Schedule")]),a("p",[e._v("第2周：中秋节放假")]),a("p",[e._v("第3-4周：搭建OpenPose所需环境：Cmake(GUI)+Visual Studio 2019+Cuda10.0")]),a("p",[e._v("第5周：国庆节放假")]),a("p",[e._v("第6-7周：建立项目并运行demo")]),a("p",[e._v("第8-10周：使用不同的图片数据集测试，发现不成功标记的例子并分类（比如遮挡、非直立、扭曲的姿势等），针对一种或几种没有成功标记的例子，思考讨论，提出可行的优化方案。")]),a("p",[e._v("第11-13周：实现优化方案的代码，给出优化后的结果")]),a("p",[e._v("第14-15周：结果汇总，制作报告")]),a("p",[e._v("第16-17周：presentation")]),a("h2",[e._v("Resources")]),a("ul",[a("li",[a("p",[e._v("Openpose项目源码：https://github.com/CMU-Perceptual-Computing-Lab/openpose")])]),a("li",[a("p",[e._v("论文地址：https://arxiv.org/pdf/1611.08050.pdf")])]),a("li",[a("p",[e._v("相关环境搭建教程：https://blog.csdn.net/qq_20226441/article/details/82380030")])]),a("li",[a("p",[e._v("图片数据库（仅供参考）：http://cocodataset.org/#download")])]),a("li",[a("p",[e._v("示例视频：https://www.youtube.com/watch?v=C1Sxk6zxWLM")])])])])}],r={components:{}},s=r,o=a("2877"),l=Object(o["a"])(s,i,n,!1,null,null,null);t["default"]=l.exports},"56d7":function(e,t,a){"use strict";a.r(t);a("8e6e"),a("ac6a"),a("456d");var i=a("bd86"),n=(a("cadf"),a("551c"),a("f751"),a("097d"),a("2b0e")),r=a("e069"),s=a.n(r),o=function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("div",{attrs:{id:"app"}},[a("Layout",[a("Header",{staticStyle:{background:"#2d8cf0","user-select":"none"}},[a("div",{staticStyle:{display:"flex","flex-direction":"row"}},[a("h1",{staticStyle:{"flex-grow":"1",color:"white"}},[e._v(e._s(e.$t("bigdataTitle")))]),a("Menu",{attrs:{mode:"horizontal",theme:"primary","active-name":"1"}},[a("MenuItem",{attrs:{name:"1",to:"/"}},[a("Icon",{attrs:{type:"ios-home"}}),e._v(e._s(e.$t("homePage"))+"\n          ")],1),a("MenuItem",{attrs:{name:"3",to:"progress"}},[a("Icon",{attrs:{type:"ios-code-working"}}),e._v(e._s(e.$t("resultList"))+"\n          ")],1),a("MenuItem",{attrs:{name:"0"}},[a("span",{staticStyle:{display:"inline-block",height:"100%"},on:{click:function(t){e.$root.$i18n.locale="en"==e.$root.$i18n.locale?"zh":"en"}}},[a("Icon",{attrs:{type:"ios-globe-outline"}}),e._v(e._s("en"==e.$i18n.locale?"Chinese":"English")+"\n            ")],1)])],1)],1)]),a("Content",{staticStyle:{padding:"48px","min-height":"calc(100vh - 133px)",margin:"0 auto","max-width":"1440px",width:"100%",background:"white"}},[a("router-view")],1),a("Footer",[a("div",[a("span",{staticStyle:{float:"left"}},[e._v("©"+e._s(e.year)+" "+e._s(e.$t("location")))]),a("a",{staticStyle:{float:"right"},attrs:{href:"https://github.com/SDU-DeepLearning"}},[a("Icon",{attrs:{type:"logo-github"}}),e._v("Github")],1)])])],1)],1)},l=[],c={data:function(){return{year:2020}},mounted:function(){this.$set(this,"year",(new Date).getFullYear())}},d=c,h=(a("034f"),a("2877")),p=Object(h["a"])(d,o,l,!1,null,null,null),u=p.exports,v=(a("7f7f"),a("8c4f"));n["default"].use(v["a"]);var m=null,g=new v["a"]({routes:[{path:"/",name:"home",meta:{title:"homePage"},component:function(){return a.e("home").then(a.bind(null,"bb51"))}},{path:"/projects",name:"projects",meta:{title:"projectList"},component:function(){return a.e("projects").then(a.bind(null,"acca"))}},{path:"/progress",name:"progress",meta:{title:"resultList"},component:function(){return a.e("progress").then(a.bind(null,"e65a"))}},{path:"/resources",name:"resources",meta:{title:"courseResources"},component:function(){return a.e("resources").then(a.bind(null,"93b9"))}},{path:"/classroom",name:"classroom",meta:{title:"courseEntries"},component:function(){return a.e("classroom").then(a.bind(null,"1636"))}}]});function f(e){m=e}g.beforeEach((function(e,t,a){document.title=m?m.t(e.meta.title)+" - "+m.t("bigdataTitle"):"神经网络与深度学习",null===g.resolve(e).resolved.name&&a("/"),a()}));var _=a("2f62");n["default"].use(_["a"]);var w=function(e){return a("c884")("./".concat(e,".md")).default},y=new _["a"].Store({state:{projects:[{id:10,name:"ChinaVis Challenge 2021",href:"",thumbnail:"/images/ChinaVis2021.png",markdown:w("ChinaVis2021")},{id:11,name:"VAST Challenge 2019 - MC2",href:"",thumbnail:"/images/VAST2019.jpeg",markdown:w("VAST2019")},{id:9,name:"VAST Challenge 2018 - MC1",href:"",thumbnail:"/images/VAST2018.jpg",markdown:w("VAST2018")},{id:12,name:"VAST Challenge 2018 - MC2",href:"",thumbnail:"/images/VAST2018_2.jpeg",markdown:w("VAST2018_2")},{id:13,name:"Visual Analytics for Nature Images",href:"",thumbnail:"/images/Nature.png",markdown:w("Nature")}],previousProjects:[{id:5,name:"VAST Challenge 2012 - MC1",href:"",thumbnail:"/images/VAST2012.jpg",markdown:w("VAST2012")},{id:6,name:"VAST Challenge 2014 - MC2",href:"",thumbnail:"/images/VAST2014.png",markdown:w("VAST2014")},{id:7,name:"VAST Challenge 2015 - MC1",href:"",thumbnail:"/images/VAST2015.jpg",markdown:w("VAST2015")},{id:8,name:"VAST Challenge 2016 - MC2",href:"",thumbnail:"/images/VAST2016.jpg",markdown:w("VAST2016")},{id:0,name:"Visual Exploration in AR",href:"",thumbnail:"/images/AR.jpeg",markdown:w("AR")},{id:1,name:"Visual Exploration in VR",href:"",thumbnail:"/images/VR.jpg",markdown:w("VR")},{id:2,name:"Visual Analytics + Business",href:"",thumbnail:"/images/business.jpg",markdown:w("business")},{id:3,name:"Sentiment Analysis",href:"",thumbnail:"/images/sentiment.png",markdown:w("sentiment")},{id:4,name:"Openpose: 2D pose estimation from single image",href:"",thumbnail:"/images/openpose.jpg",markdown:w("openpose")}],classrooms:[{id:0,name:"2019年秋季班级",enName:"Autumn class, 2021",entry:"https://classroom.github.com/classrooms/55434637-sdubigdatacourse-2019",repos:[{name:"文献阅读与展示",enName:"Presentation",markdown:w("PaperReading")}]}],references:[{id:0,name:"Deep Learning",translateName:"深度学习",authors:"Ian Goodfellow, Yoshua Bengio, and Aaron Courville",pub:"MIT",pubI18n:!1,pubYear:2016},{id:1,name:"Deep Learning Methods and Applications",translateName:"深度学习方法与应用",authors:"Li Deng and Dong Yu",pub:"Now Publisher",pubI18n:!1,pubYear:2014},{id:2,name:"Learning Deep Architectures for AI",translateName:"学习人工智能的深度架构",authors:"Yoshua Bengio",pub:"Now Publisher",pubI18n:!1,pubYear:2009},{id:3,name:"Machine Learning",translateName:"机器学习",authors:"Zhou Zhihua",pub:"Tsinghua University",pubI18n:!1,pubYear:2016}]}}),b=a("37e1"),k=a.n(b),A=a("a925"),C=a("69c5"),S=a.n(C),T=a("9ef8"),E=a.n(T),L=a("edd4"),P=a("3504");a("dcad");function x(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);t&&(i=i.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,i)}return a}function D(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?x(a,!0).forEach((function(t){Object(i["a"])(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):x(a).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}n["default"].config.productionTip=!1,n["default"].use(A["a"]),n["default"].use(s.a);var R=new A["a"]({locale:"en",messages:{en:D({},S.a,{},L),zh:D({},E.a,{},P)}});f(R),new n["default"]({router:g,store:y,render:function(e){return e(u)},i18n:R}).$mount("#app"),k.a.init()},"5b46":function(e,t,a){"use strict";a.r(t);var i=function(){var e=this,t=e.$createElement;e._self._c;return e._m(0)},n=[function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("section",[a("h2",[e._v("Challenge Descriptions")]),a("p",[e._v("The setting for the 2012 VAST Challenges is BankWorld, a planet much like Earth, but with a very different geography. In fact, for this Challenge, we are dealing with one large land mass containing several different nation-states.")]),a("p",[e._v("The most important organization on BankWorld is the Bank of Money (BOM). BOM has many offices of various sizes across BankWorld. Each of these offices has many computers active throughout the day. In fact, we are dealing with about 1,000,000 machines.")]),a("p",[e._v("The Bank of Money (BOM) Corporate Information Officer (CIO) has assigned you to create a situation awareness visualization of the entire enterprise. This is a considerable challenge, considering that BOM operates from BankWorld's coast to coast. In addition to observing the global situation, he would also would like to be able to detect operational changes outside of the norm.")]),a("p",[e._v("You are provided with two datasets that span two days of data for BOM.\nOne dataset contains metadata about the bank’s network.\nThe second dataset contains periodic status reports from all computing equipment in the BOM enterprise.")]),a("p",[e._v("There is also one additional smaller dataset that contains a one hour snapshot of the enterprise's activities. It has the same format as the second dataset mentioned above, and can use the metadata contained in the first dataset.")]),a("h2",[e._v("Project Task")]),a("ul",[a("li",[a("p",[e._v("Create a visualization of the health and policy status of the entire Bank of Money enterprise as of 2 pm BMT (BankWorld Mean Time) on February 2. What areas of concern do you observe?")])]),a("li",[a("p",[e._v("Use your visualization tools to look at how the network’s status changes over time. Highlight up to five potential anomalies in the network and provide a visualization of each. When did each anomaly begin and end? What might be an explanation of each anomaly?")])])]),a("h2",[e._v("Schedule")]),a("ul",[a("li",[a("p",[e._v("Fetch Dataset, analyse tasks, and make a plan to solve it (2-3 week)")])]),a("li",[a("p",[e._v("Process data, implement analysis framework (4-7 week)")])]),a("li",[a("p",[e._v("Mid-term inspection (8 week)")])]),a("li",[a("p",[e._v("Implement analysis system (9-12 week)")])]),a("li",[a("p",[e._v("Inter-team evaluation and modification (13-15 week)")])]),a("li",[a("p",[e._v("Final inspection (16-17 week)")])])]),a("h2",[e._v("Resources")]),a("ul",[a("li",[e._v("http://www.vacommunity.org/VAST+Challenge+2012%3A+Challenge+Descriptions")])])])}],r={components:{}},s=r,o=a("2877"),l=Object(o["a"])(s,i,n,!1,null,null,null);t["default"]=l.exports},"5fbb":function(e,t,a){"use strict";a.r(t);var i=function(){var e=this,t=e.$createElement;e._self._c;return e._m(0)},n=[function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("section",[a("h2",[e._v("Introduction")]),a("p",[e._v("Virtual reality (VR) is a computer-generated model of experiencing in which, although the surrounding is not real in the technical sense, they appear to be so. Creating immersive visualizations remains challenging, and often require complex low-level programming and tedious manual encoding of data attributes to geometric and visual properties.")]),a("p",[e._v("Advantages of VR for visual exploration:")]),a("ol",[a("li",[a("p",[e._v("More data visualization possibilities: many more dimensions than the traditional placement coordinates (X,Y, and Z) become available. It can even be classified according to direction or magnitude of a vector.")])]),a("li",[a("p",[e._v("Intuitive approach: the way VR will present data is the way we interact with the world at large.")])]),a("li",[a("p",[e._v("Multiple users: when data is presented in VR, multiple users can inhabit the environment at the same time.")])]),a("li",[a("p",[e._v("Eliminating distractions: with a user tapped into data presented in VR, their visual and to some extent aural senses are completely governed by the virtual environment.")])])]),a("p",[e._v("This project will be built on DXR toolkit. DXR is help developers efficiently specify visualization designs using a concise declarative visualization grammar inspired by Vega-Lite. A GUI is provided for easy and quick edits and previews of visualization designs in-situ. Reusable templates and customizable graphical marks are also provided to enable unique and engaging visualizations.")]),a("p",[e._v("A typical DXR toolkit system scenario is as below: (1) the designer describes the visualization design in a concise specification (vis-specs) using DXR’s high-level visualization grammar. (2) DXR then infers missing visualization parameters with sensible defaults. (3) Based on this complete specification, DXR then programmatically constructs the 3D visualization that the designer can place in a real or virtual immersive scene.")]),a("h2",[e._v("Project Task")]),a("ul",[a("li",[a("p",[e._v("Run demos and examples in a VR environment.")])]),a("li",[a("p",[e._v("Summarize the shortcomings of visualizations in this toolkit and consider methods to improve them.")])])]),a("h2",[e._v("Schedule")]),a("ul",[a("li",[a("p",[e._v("Reproduce the paper result (3-5 week)")])]),a("li",[a("p",[e._v("Find some patterns in a dataset (6-7 week)")])]),a("li",[a("p",[e._v("Conclude the shortcoming of the paper in VR scenarios (6-7 week)")])]),a("li",[a("p",[e._v("Mid-term inspection (8 week)")])]),a("li",[a("p",[e._v("Find a way to optimize the toolkit (9-15 week)")])]),a("li",[a("p",[e._v("Final inspection (16-17 week)")])])]),a("h2",[e._v("Implementation tools")]),a("ul",[a("li",[a("p",[e._v("DXR")])]),a("li",[a("p",[e._v("HTC Vive")])]),a("li",[a("p",[e._v("Unity Editor")])])]),a("h2",[e._v("Resources")]),a("ul",[a("li",[a("p",[e._v("Paper: https://vcg.seas.harvard.edu/publications/dxr-a-toolkit-for-building-immersive-data-visualizations")])]),a("li",[a("p",[e._v("Source Code: https://github.com/ronellsicat/DxR")])]),a("li",[a("p",[e._v("Datasets:")]),a("ul",[a("li",[e._v("DXR Data (included in source): https://github.com/ronellsicat/DxR/tree/master/Assets/StreamingAssets/DxRData")]),a("li",[e._v("IATK Data: https://github.com/MaximeCordeil/IATK/tree/master/Assets/Datasets")])])])])])}],r={components:{}},s=r,o=a("2877"),l=Object(o["a"])(s,i,n,!1,null,null,null);t["default"]=l.exports},"64a9":function(e,t,a){},"6f20":function(e,t,a){"use strict";a.r(t);var i=function(){var e=this,t=e.$createElement;e._self._c;return e._m(0)},n=[function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("section",[a("h2",[e._v("Challenge Descriptions")]),a("p",[e._v("Mistford is a mid-size city located to the southwest of the Boonsong Lekagul Wildlife Preserve. The city has a small industrial area with four light-manufacturing endeavors. Mistford and the wildlife preserve are struggling with the possible endangerment of the Rose-Crested Blue Pipit, a locally loved bird. The bird’s nesting pairs seem to have decreased alarmingly, prompting an investigation last year implicating Kasios Office Furniture, a Mistford manufacturing firm. Since the initial investigation, the situation has evolved: Kasios insists that they have done nothing wrong! They assert that grad student Mitch Vogel and his professors are mere media-seekers trying to draw attention away from their lackadaisical research. Kasios presents itself as an extremely eco-friendly organization. They have launched their own very public investigation into the issues raised last year and are reporting very different results! It’s time to apply your visual analytics expertise to help illuminate the path to good science.")]),a("p",[e._v("The scenario for this challenge builds on last year’s but the challenges are new. Please see the VAST Challenge 2017 for additional background on this year's challenge! Readings from across the mini-challenges will help you with this year's effort. This year's scenario (the setting, the geography, the situation) are carried over to this year, however, you do not have to have participated in last year's challenge to work on this year's.")]),a("p",[e._v("In 2017, the VAST Challenge results suggested that the Kasios Furniture manufacturing company may have been a primary contributor to the apparent reduction of the number of nesting pairs of the Rose-Crested Blue Pipit, a favorite bird of Mistford residents and Boonsong Lekagul Nature Preserve visitors. Kasios supposedly used the banned substance Methylosmolene in their manufacturing process. They surreptitiously dumped process waste in the northeast region of the Preserve (mini-challenge 1 from 2017) and Methylosmolene was detected in their smokestack emissions (mini-challenge 2 from 2017).")]),a("p",[e._v("Kasios now claims that the analysis was flawed and biased. To combat these conclusions, Kasios has launched their own “investigation” into the Pipit situation, and they are now reporting that there are plenty of Rose-crested Blue Pipits happily living and nesting in the Preserve. To back up this claim, they have provided a set of Pipit bird calls, recently recorded across the Preserve, with locations of where they were recorded. Clearly, they claim, the Pipits are a thriving population, and Kasios will provide even more supporting evidence as their investigation proceeds.")]),a("p",[e._v("In last year’s Challenge, an ornithology grad student from Mistford College named Mitch Vogel discovered the plight of the Pipit and carried out an investigation. Normally, we would call on Mitch again to help validate Kasios’ claim. Unfortunately, Mitch is working far from Mistford in a remote area without internet access for an extended time and cannot be easily reached!")]),a("p",[e._v("The Pangera Ornithology Conservation Society, who sponsored Mitch last year, is at their wit’s end at what to do about this turn of events. The townsfolk and Preserve rangers seem satisfied that the recordings back up Kasios’ claims. Mistford College does not have another Pipit expert they can call upon for help. But, they do have a collection of bird calls from the Preserve that has been vetted by various ornithology groups as having accurate identifications. They have heard that new techniques from machine learning and visual analytics can be applied to situations like this. Perhaps, the calls could be classified and analyzed using these technologies, and reviewed when Mitch returns.")]),a("h2",[e._v("Project Task")]),a("ul",[a("li",[a("p",[e._v("Using the bird call collection and the included map of the Wildlife Preserve, characterize the patterns of all of the bird species in the Preserve over the time of the collection. Please assume we have a reasonable distribution of sensors and human collectors providing the recordings, so that the patterns are reasonably representative of the bird locations across the area. Do you detect any trends or anomalies in the patterns?")])]),a("li",[a("p",[e._v("Turn your attention to the set of bird calls supplied by Kasios. Does this set support the claim of Pipits being found across the Preserve? A machine learning approach using the bird call library may help your investigation. What is the role of visualization in your analysis of the Kasios bird calls?")])]),a("li",[a("p",[e._v("Formulate a hypotheses concerning the state of the Rose Crested Blue Pipit. What are your primary pieces of evidence to support your assertion? What next steps should be taken in the investigation to either support or refute the Kasios claim that the Pipits are actually thriving across the Boonsong Lekagul Wildlife Preserve?")])])]),a("h2",[e._v("Schedule")]),a("ul",[a("li",[a("p",[e._v("Fetch Dataset, analyse tasks, and make a plan to solve it (2-3 week)")])]),a("li",[a("p",[e._v("Process data, implement analysis framework (4-7 week)")])]),a("li",[a("p",[e._v("Mid-term inspection (8 week)")])]),a("li",[a("p",[e._v("Implement analysis system (9-12 week)")])]),a("li",[a("p",[e._v("Inter-team evaluation and modification (13-15 week)")])]),a("li",[a("p",[e._v("Final inspection (16-17 week)")])])]),a("h2",[e._v("Resources")]),a("ul",[a("li",[e._v("http://vacommunity.org/VAST+Challenge+2018+MC1")])])])}],r={components:{}},s=r,o=a("2877"),l=Object(o["a"])(s,i,n,!1,null,null,null);t["default"]=l.exports},"7f89":function(e,t,a){"use strict";a.r(t);var i=function(){var e=this,t=e.$createElement;e._self._c;return e._m(0)},n=[function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("section",[a("h2",[e._v("Challenge Descriptions")]),a("p",[e._v("在nature上我们有很多论文的配图，这些配图我们很容易通过一些爬虫的技术来获取到。对于nature论文上的配图，我们可以分析他们有哪些特征，以及子类别下有什么共同趋势。例如"),a("a",{attrs:{href:"https://quickdraw.withgoogle.com/data"}},[e._v("Google Creative Lab")]),e._v("，提供了人们对物体的手绘。对于同一种物体，不同的人会有不同的绘制喜好。即使是同一个物体，可以按不同笔画数目进行分类，区别也非常明显。甚至于不同的国家的人，对绘制的偏好也有所不同。你是否可以以此为启发，对nature的论文配图做出类似的分析。")]),a("h2",[e._v("Project Task")]),a("ul",[a("li",[e._v("Web crawler")]),a("li",[e._v("Data annotation")]),a("li",[e._v("Feature Learning")]),a("li",[e._v("Clustering / Embedding")]),a("li",[e._v("Statistical analysis")]),a("li",[e._v("Visualization")])]),a("h2",[e._v("Schedule")]),a("ul",[a("li",[a("p",[e._v("Fetch Dataset, analyse tasks, and make a plan to solve it (2-3 week)")])]),a("li",[a("p",[e._v("Process data, implement analysis framework (4-7 week)")])]),a("li",[a("p",[e._v("Mid-term inspection (8 week)")])]),a("li",[a("p",[e._v("Implement analysis system (9-12 week)")])]),a("li",[a("p",[e._v("Inter-team evaluation and modification (13-15 week)")])]),a("li",[a("p",[e._v("Final inspection (16-17 week)")])])]),a("h2",[e._v("Resources")]),a("ul",[a("li",[e._v("https://vast-challenge.github.io/2019/MC2.html")])])])}],r={components:{}},s=r,o=a("2877"),l=Object(o["a"])(s,i,n,!1,null,null,null);t["default"]=l.exports},a2b5:function(e,t,a){"use strict";a.r(t);var i=function(){var e=this,t=e.$createElement;e._self._c;return e._m(0)},n=[function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("section",[a("h2",[e._v("Challenge Descriptions")]),a("p",[e._v("Last year, the Kasios Furniture Company was implicated in environmental damage to the Boonsong Lekagul Wildlife Preserve for both dumping toxic waste and polluting the air with chemicals from its manufacturing process. But Kasios is not taking these accusations lying down, and they deny any accusation of industrial waste dumping! Kasios’ spokespersons state that there isn’t any ground contamination near the remote ranger station that was suggested by last year’s mini-challenge 1 and 3 participants, and they have inspected that area and found it as pristine as the rest of the preserve.")]),a("p",[e._v("Outraged ornithology professors from Mistford College journeyed out to look over the dumping site themselves and perform soil analyses. They returned to report that the site looked like there had been recent excavation and building activities going on. Boonsong Preserve rangers later confirmed that a new ranger station was being built at that site! Soil samples taken from the site were inconclusive in detecting Methylosmolene (the toxic manufacturing chemical in the suspected dumping) or any other contaminant, as new top soil had been trucked in.")]),a("p",[e._v("With a primary piece of evidence against Kasios now gone, investigators will need to take another approach. Professors in the Mistford College Hydrology Department have come forward with several years of water sensor readings from rivers and streams in the preserve. These samples were taken from different locations scattered throughout the area and contain measurements of several chemicals of possible interest, but they have never been analyzed due to lack of funding. Could visual analytics help reveal something in this data that could make up for the soil evidence that was destroyed?")]),a("p",[e._v("Your task is to investigate the hydrology data from across the Preserve. You are given a map of the Preserve (with the same base as last year’s challenge), with named sampling sites indicated on the map (the names have local significance, but are just mnemonics for your study) You are also provided with readings from each sampling station over time for several different chemicals and water properties.")]),a("h2",[e._v("Project Task")]),a("ul",[a("li",[e._v("Characterize the past and most recent situation with respect to chemical contamination in the Boonsong Lekagul waterways. Do you see any trends of possible interest in this investigation?")]),a("li",[e._v("What anomalies do you find in the waterway samples dataset? How do these affect your analysis of potential problems to the environment? Is the Hydrology Department collecting sufficient data to understand the comprehensive situation across the Preserve? What changes would you propose to make in the sampling approach to best understand the situation?")]),a("li",[e._v("After reviewing the data, do any of your findings cause particular concern for the Pipit or other wildlife? Would you suggest any changes in the sampling strategy to better understand the waterways situation in the Preserve?")])]),a("h2",[e._v("Schedule")]),a("ul",[a("li",[a("p",[e._v("Fetch Dataset, analyse tasks, and make a plan to solve it (2-3 week)")])]),a("li",[a("p",[e._v("Process data, implement analysis framework (4-7 week)")])]),a("li",[a("p",[e._v("Mid-term inspection (8 week)")])]),a("li",[a("p",[e._v("Implement analysis system (9-12 week)")])]),a("li",[a("p",[e._v("Inter-team evaluation and modification (13-15 week)")])]),a("li",[a("p",[e._v("Final inspection (16-17 week)")])])]),a("h2",[e._v("Resources")]),a("ul",[a("li",[e._v("http://vacommunity.org/VAST+Challenge+2018+MC2")])])])}],r={components:{}},s=r,o=a("2877"),l=Object(o["a"])(s,i,n,!1,null,null,null);t["default"]=l.exports},a5a9:function(e,t,a){"use strict";a.r(t);var i=function(){var e=this,t=e.$createElement;e._self._c;return e._m(0)},n=[function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("section",[a("h2",[e._v("Challenge Descriptions")]),a("p",[e._v("目前，我国大气污染防治成效显著，这得益于我国逐渐完善的空气质量监测网。近年来，空气质量监测站收集到大量具有高维、时序特点的空气质量数据，如何利用此类数据，分析理解大气污染传输模式，并为决策者提供有效建议十分具有挑战性。利用大数据分析技术和可视化方法，能够分析大气污染问题及成因、监测大气污染发展趋势、分析大气污染的地域相关性，快速感知大气污染的时变规律，辅助工作人员因地制宜地制定防治策略。大数据可视分析与可视化将数据智能处理、视觉表征和交互分析有机地结合，使机器智能和人类智慧深度融合、优势互补，为大气污染防治工作的分析、指挥和决策提供有效手段和决策依据。第八届中国可视化与可视分析大会数据可视化竞赛向相关研究人员、高校师生、企业和数据可视分析爱好者、艺术家征集相关数据可视分析和艺术可视化作品。")]),a("h2",[e._v("Project Task")]),a("p",[e._v("参赛作品要求利用竞赛提供的2013–2018年中国高分辨率大气污染再分析开放数据集（http://naq.cicidata.top:10443/chinavis/opendata），利用可视分析技术与可视化方法，探索并发现空气质量大数据背后隐藏的模式和规律。该数据是覆盖全国范围的基于地理空间网格的空气质量再分析数据和对应的气象数据，包括六项常规污染物、风速、温度、气压、相对湿度和经纬度在内的13个属性（详细数据说明请登录开放数据集网站）。")]),a("ul",[a("li",[e._v("大气污染源分析：利用可视分析技术，识别主要大气污染源，分析关键污染成因。（可以根据自身情况联合其他数据辅助分析）")]),a("li",[e._v("大气污染时空态势分析：利用可视分析技术，分析大气污染时空分布模式、监控大气污染时空演变态势。")]),a("li",[e._v("大气污染传输模式分析：利用可视分析技术，比较各地大气污染物差异、大气污染传输模式、检测异常传输事件，制定传输防治策略。")]),a("li",[e._v("大气污染预测：利用可视分析技术，预测大气污染发展趋势、重污染天气预警。")]),a("li",[e._v("大气环境的改善：利用可视分析技术，展示大气污染治理过程中的大气环境状况、评估大气污染防治措施。")]),a("li",[e._v("自选主题，提炼分析需求，设置分析问题，并提供解决方案。")])]),a("h2",[e._v("Schedule")]),a("ul",[a("li",[a("p",[e._v("Fetch Dataset, analyse tasks, and make a plan to solve it (2-3 week)")])]),a("li",[a("p",[e._v("Process data, implement analysis framework (4-7 week)")])]),a("li",[a("p",[e._v("Mid-term inspection (8 week)")])]),a("li",[a("p",[e._v("Implement analysis system (9-12 week)")])]),a("li",[a("p",[e._v("Inter-team evaluation and modification (13-15 week)")])]),a("li",[a("p",[e._v("Final inspection (16-17 week)")])])]),a("h2",[e._v("Resources")]),a("ul",[a("li",[e._v("http://www.chinavis.org/2021/challenge.html")])])])}],r={components:{}},s=r,o=a("2877"),l=Object(o["a"])(s,i,n,!1,null,null,null);t["default"]=l.exports},b732:function(e,t,a){"use strict";a.r(t);var i=function(){var e=this,t=e.$createElement;e._self._c;return e._m(0)},n=[function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("section",[a("table",[a("tr",[a("td",{attrs:{width:"10%",align:"center"}},[e._v("Topic")]),a("td",{attrs:{width:"70%",align:"center"}},[e._v("Paper")]),a("td",{attrs:{width:"10%",align:"center"}},[e._v("Paper Link")]),a("td",{attrs:{width:"10%",align:"center"}},[e._v("Slide Link")])]),a("tr",[a("td",{attrs:{width:"10%",align:"center"}},[e._v("CNN")]),a("td",{attrs:{width:"70%",align:"center"}},[e._v("Very deep convolutional networks for large-scale image recognition")]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://arxiv.org/pdf/1409.1556.pdf%E3%80%82"}},[e._v("Link")])]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://drive.google.com/file/d/1tEcnU8PSvEETpPfLx90JRToIIU96EgE0/view?usp=sharing"}},[e._v("Link")])])]),a("tr",[a("td",{attrs:{width:"10%",align:"center"}},[e._v("CNN")]),a("td",{attrs:{width:"70%",align:"center"}},[e._v("Deep residual learning for image recognition")]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf"}},[e._v("Link")])]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://drive.google.com/file/d/1AuluiDAzikr_Bp1O5GYkj-aOZAf6OpbM/view?usp=sharing"}},[e._v("Link")])])]),a("tr",[a("td",{attrs:{width:"10%",align:"center"}},[e._v("CNN")]),a("td",{attrs:{width:"70%",align:"center"}},[e._v("Densely connected convolutional networks")]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.pdf"}},[e._v("Link")])]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://drive.google.com/file/d/1SgKjGB0Fd-mn5RexCEVpHp0uDEK6s7Vg/view?usp=sharing"}},[e._v("Link")])])]),a("tr",[a("td",{attrs:{width:"10%",align:"center"}},[e._v("Object Detection")]),a("td",{attrs:{width:"70%",align:"center"}},[e._v("Rich feature hierarchies for accurate object detection and semantic segmentation")]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://openaccess.thecvf.com/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf"}},[e._v("Link")])]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://drive.google.com/file/d/12eLTY6u9DlfWn2dLYc4YGh4iwY2tLRy1/view?usp=sharing"}},[e._v("Link")])])]),a("tr",[a("td",{attrs:{width:"10%",align:"center"}},[e._v("RNN")]),a("td",{attrs:{width:"70%",align:"center"}},[e._v("Neural Machine Translation by Jointly Learning to Align and Translate")]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://arxiv.org/pdf/1409.0473.pdf?utm_source=ColumnsChannel"}},[e._v("Link")])]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://drive.google.com/file/d/1LRw8eGnddFUIwjuDLkyTW-VX7qYNMssi/view?usp=sharing"}},[e._v("Link")])])]),a("tr",[a("td",{attrs:{width:"10%",align:"center"}},[e._v("RNN")]),a("td",{attrs:{width:"70%",align:"center"}},[e._v("Generating Sequences with Recurrent Neural Networks")]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://arxiv.org/pdf/1308.0850"}},[e._v("Link")])]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://drive.google.com/file/d/1GUjH4mWpXvFhiLixxLIW-4xhrXcvg3j7/view?usp=sharing"}},[e._v("Link")])])]),a("tr",[a("td",{attrs:{width:"10%",align:"center"}},[e._v("RNN")]),a("td",{attrs:{width:"70%",align:"center"}},[e._v("Sequence to Sequence Learning with Neural Networks")]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://proceedings.neurips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf"}},[e._v("Link")])]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://drive.google.com/file/d/1s4yiAS-yVDIA3_md9eKUIrw_cRNJjx7T/view?usp=sharing"}},[e._v("Link")])])]),a("tr",[a("td",{attrs:{width:"10%",align:"center"}},[e._v("Attention")]),a("td",{attrs:{width:"70%",align:"center"}},[e._v("Attention is All You Need")]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf"}},[e._v("Link")])]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://drive.google.com/file/d/1uPZv5JQiMe6mjjvPqcKKAdDJU5fh9otU/view?usp=sharing"}},[e._v("Link")])])]),a("tr",[a("td",{attrs:{width:"10%",align:"center"}},[e._v("Attention")]),a("td",{attrs:{width:"70%",align:"center"}},[e._v("Look Closer to See Better: Recurrent Attention Convolutional NeuralNetwork for Fine-grained lmage Recognition")]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://openaccess.thecvf.com/content_cvpr_2017/papers/Fu_Look_Closer_to_CVPR_2017_paper.pdf"}},[e._v("Link")])]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://drive.google.com/file/d/1vENf-cOwmtWkaQjLIF4rq-gSF6pxsfKI/view?usp=sharing"}},[e._v("Link")])])]),a("tr",[a("td",{attrs:{width:"10%",align:"center"}},[e._v("Attention")]),a("td",{attrs:{width:"70%",align:"center"}},[e._v("Show, Attend and Tell: Neural Image Caption Generation with Visual Attention")]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"http://proceedings.mlr.press/v37/xuc15.pdf"}},[e._v("Link")])]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://drive.google.com/file/d/11d9rnNigBLQS63A8UnByhsWEb0ykmpvX/view?usp=sharing"}},[e._v("Link")])])]),a("tr",[a("td",{attrs:{width:"10%",align:"center"}},[e._v("Vision & Language")]),a("td",{attrs:{width:"70%",align:"center"}},[e._v("VL-BERT: Pre-training of Generic Visual-Linguistic Representations")]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://arxiv.org/pdf/1908.08530.pdf"}},[e._v("Link")])]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://drive.google.com/file/d/1qye2jLDFERpQ6V1XAyYCeoBas6LhKSGu/view?usp=sharing"}},[e._v("Link")])])]),a("tr",[a("td",{attrs:{width:"10%",align:"center"}},[e._v("Vision & Language")]),a("td",{attrs:{width:"70%",align:"center"}},[e._v("Stacked Cross Attention for Image-Text Matching")]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://openaccess.thecvf.com/content_ECCV_2018/papers/Kuang-Huei_Lee_Stacked_Cross_Attention_ECCV_2018_paper.pdf"}},[e._v("Link")])]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://drive.google.com/file/d/1z-2-pym-KeyYG3W_xL1OExAvmlGI4iRt/view?usp=sharing"}},[e._v("Link")])])]),a("tr",[a("td",{attrs:{width:"10%",align:"center"}},[e._v("Transfer Learning")]),a("td",{attrs:{width:"70%",align:"center"}},[e._v("Deep Domain Confusion: Maximizing for Domain Invariance")]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://arxiv.org/pdf/1412.3474.pdf"}},[e._v("Link")])]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://drive.google.com/file/d/1_Vm6ulAFgUrLgMrPSMFiQ6lW5P0zV1aU/view?usp=sharing"}},[e._v("Link")])])]),a("tr",[a("td",{attrs:{width:"10%",align:"center"}},[e._v("Transfer Learning")]),a("td",{attrs:{width:"70%",align:"center"}},[e._v("A Survey on Transfer Learning")]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://www.cse.ust.hk/~qyang/Docs/2009/tkde_transfer_learning.pdf"}},[e._v("Link")])]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://drive.google.com/file/d/1kzNplMyxthSkJRa9_y6THcXUeWLPb-mB/view?usp=sharing"}},[e._v("Link")])])]),a("tr",[a("td",{attrs:{width:"10%",align:"center"}},[e._v("GAN")]),a("td",{attrs:{width:"70%",align:"center"}},[e._v("Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks")]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://arxiv.org/pdf/1511.06434.pdf%C3"}},[e._v("Link")])]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://drive.google.com/file/d/1X67MOQPe-JC2KYBDX-Nb2B4qyWBVGCnB/view?usp=sharing"}},[e._v("Link")])])]),a("tr",[a("td",{attrs:{width:"10%",align:"center"}},[e._v("GAN")]),a("td",{attrs:{width:"70%",align:"center"}},[e._v("Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks")]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://openaccess.thecvf.com/content_ICCV_2017/papers/Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper.pdf"}},[e._v("Link")])]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://drive.google.com/file/d/1WVLRDoT_GPmqV1EFJkW0t_vF1MfzFylq/view?usp=sharing"}},[e._v("Link")])])]),a("tr",[a("td",{attrs:{width:"10%",align:"center"}},[e._v("DRL")]),a("td",{attrs:{width:"70%",align:"center"}},[e._v("Playing atari with deep reinforcement learning")]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://arxiv.org/pdf/1312.5602.pdf?source=post_page---------------------------"}},[e._v("Link")])]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://drive.google.com/file/d/1aGwJDQN-3ZR6-dTnyPMICsfohftUq5xI/view?usp=sharing"}},[e._v("Link")])])]),a("tr",[a("td",{attrs:{width:"10%",align:"center"}},[e._v("DRL")]),a("td",{attrs:{width:"70%",align:"center"}},[e._v("Mastering the game of Go with deep neural networks and tree search")]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://hci.iwr.uni-heidelberg.de/system/files/private/downloads/36349047/report_florian_brunner.pdf"}},[e._v("Link")])]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://drive.google.com/file/d/1aLvnwjynZt7e9ubaI7lhUy3H8bJVy6ED/view?usp=sharing"}},[e._v("Link")])])]),a("tr",[a("td",{attrs:{width:"10%",align:"center"}},[e._v("Application")]),a("td",{attrs:{width:"70%",align:"center"}},[e._v("Unified language model pre-training for natural language understanding and generation")]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://proceedings.neurips.cc/paper/2019/file/c20bb2d9a50d5ac1f713f8b34d9aac5a-Paper.pdf"}},[e._v("Link")])]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://drive.google.com/file/d/17Ej8X8Zra-rcBTrV6cs-Tu7M1NdD4oE9/view?usp=sharing"}},[e._v("Link")])])]),a("tr",[a("td",{attrs:{width:"10%",align:"center"}},[e._v("Application")]),a("td",{attrs:{width:"70%",align:"center"}},[e._v("Speech recognition with deep recurrent neural networks")]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://arxiv.org/pdf/1303.5778.pdf%C3%AF%C2%BC%E2%80%B0%C3%AF%C2%BC%C5%A1%E2%80%9C%C3%A5%C2%A6%E2%80%9A%C3%A6%C5%BE%C5%93LSTM%C3%A7%E2%80%9D%C2%A8%C3%A4%C2%BA%C5%BD%C3%A9%C5%A1%20%C3%A8%E2%80%94%20%C3%A5%C2%B1%E2%80%9A%C3%AF%C2%BC%C5%92%C3%A6%CB%86%E2%80%98%C3%A4%C2%BB%C2%AC%C3%A5%C2%B0%E2%80%A0%C3%A5%C2%BE%E2%80%94%C3%A5%CB%86%C2%B0"}},[e._v("Link")])]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://drive.google.com/file/d/1ootKrLoouixWVGiC1Mg-qZ1IJ3WfeCDM/view?usp=sharing"}},[e._v("Link")])])]),a("tr",[a("td",{attrs:{width:"10%",align:"center"}},[e._v("GNN")]),a("td",{attrs:{width:"70%",align:"center"}},[e._v("Inductive Representation Learning on Large Graphs")]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://proceedings.neurips.cc/paper/2017/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf"}},[e._v("Link")])]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://drive.google.com/file/d/17F1iNPLoTs0Lln5zPee5HFk5sKA7o_ks/view?usp=sharing"}},[e._v("Link")])])]),a("tr",[a("td",{attrs:{width:"10%",align:"center"}},[e._v("GNN")]),a("td",{attrs:{width:"70%",align:"center"}},[e._v("Simplifying Graph Convolutional Networks")]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"http://proceedings.mlr.press/v97/wu19e/wu19e.pdf"}},[e._v("Link")])]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://drive.google.com/file/d/1quVcj-Ivp3jayzre5ncgmk9q16ezzdQf/view?usp=sharing"}},[e._v("Link")])])]),a("tr",[a("td",{attrs:{width:"10%",align:"center"}},[e._v("Segmentation")]),a("td",{attrs:{width:"70%",align:"center"}},[e._v("U-net: Convolutional networks for biomedical image segmentation")]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://link.springer.com/content/pdf/10.1007/978-3-319-24574-4_28.pdf"}},[e._v("Link")])]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://drive.google.com/file/d/12U2aCGG1Jk_B0_i2WHTCc3FFYn9Q5JqA/view?usp=sharing"}},[e._v("Link")])])]),a("tr",[a("td",{attrs:{width:"10%",align:"center"}},[e._v("Segmentation")]),a("td",{attrs:{width:"70%",align:"center"}},[e._v("Fully convolutional networks for semantic segmentation")]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://openaccess.thecvf.com/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf"}},[e._v("Link")])]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://drive.google.com/file/d/1tbvGTk1h4i85us3C4r-iZRVHE8bF3bxH/view?usp=sharing"}},[e._v("Link")])])]),a("tr",[a("td",{attrs:{width:"10%",align:"center"}},[e._v("Object Detection")]),a("td",{attrs:{width:"70%",align:"center"}},[e._v("YOLOv3: An Incremental Improvement")]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://arxiv.org/pdf/1804.02767.pdf;"}},[e._v("Link")])]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://drive.google.com/file/d/1ebuajd3RenZVfou3czunIcS1oh8f3vTD/view?usp=sharing"}},[e._v("Link")])])]),a("tr",[a("td",{attrs:{width:"10%",align:"center"}},[e._v("Tracking")]),a("td",{attrs:{width:"70%",align:"center"}},[e._v("SiameseFC：Fully-Convolutional Siamese Networks for Object Tracking")]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://link.springer.com/chapter/10.1007/978-3-319-48881-3_56"}},[e._v("Link")])]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://drive.google.com/file/d/1rxHpRCSCBJOOuTWl51PIyPzVbXneT--V/view?usp=sharing"}},[e._v("Link")])])]),a("tr",[a("td",{attrs:{width:"10%",align:"center"}},[e._v("Application")]),a("td",{attrs:{width:"70%",align:"center"}},[e._v("Neural Collaborative Filtering")]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://arxiv.org/pdf/1708.05031.pdf%E4%B8%AD%E9%A6%96%E5%85%88%E8%A2%AB%E6%8F%90%E5%87%BA%E6%9D%A5%E3%80%82%E8%AE%BA%E6%96%87%E5%81%87%E8%AE%BE%E5%A6%82%E6%9E%9C%E7%94%A8%E6%88%B7u%E8%B4%AD%E4%B9%B0%E4%BA%86%E7%89%A9%E5%93%81i,%E5%88%99yui=1%E5%90%A6%E5%88%99yui=0%EF%BC%8C%E5%88%99%E6%9C%80%E7%BB%88%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BC%98%E5%8C%96%E7%9B%AE%E6%A0%87%E4%B8%BA"}},[e._v("Link")])]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://drive.google.com/file/d/1AeQpu_Z6kUSQ8TD1zqUw-3WJWiyXtvn_/view?usp=sharing"}},[e._v("Link")])])]),a("tr",[a("td",{attrs:{width:"10%",align:"center"}},[e._v("Application")]),a("td",{attrs:{width:"70%",align:"center"}},[e._v("A Discriminatively Learned CNN Embedding for Person Re-identification")]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://arxiv.org/pdf/1611.05666.pdf"}},[e._v("Link")])]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://drive.google.com/file/d/1sGxH3RRvy9SL5AxWl81Cpqm07yMd64FL/view?usp=sharing"}},[e._v("Link")])])]),a("tr",[a("td",{attrs:{width:"10%",align:"center"}},[e._v("Application")]),a("td",{attrs:{width:"70%",align:"center"}},[e._v("Image Super-Resolution Using Very Deep Residual Channel Attention Networks")]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://openaccess.thecvf.com/content_ECCV_2018/papers/Yulun_Zhang_Image_Super-Resolution_Using_ECCV_2018_paper.pdf"}},[e._v("Link")])]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://drive.google.com/file/d/1Gf1FmqwUlJf4NDRSMbuoUdjlu7Pipkx5/view?usp=sharing"}},[e._v("Link")])])]),a("tr",[a("td",{attrs:{width:"10%",align:"center"}},[e._v("Video QA")]),a("td",{attrs:{width:"70%",align:"center"}},[e._v("Hierarchical Conditional Relation Networks for Video Question Answering")]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://openaccess.thecvf.com/content_CVPR_2020/papers/Le_Hierarchical_Conditional_Relation_Networks_for_Video_Question_Answering_CVPR_2020_paper.pdf"}},[e._v("Link")])]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:""}},[e._v("Link")])])]),a("tr",[a("td",{attrs:{width:"10%",align:"center"}},[e._v("Scene Segmentation")]),a("td",{attrs:{width:"70%",align:"center"}},[e._v("Dual Attention Network for Scene Segmentation")]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://openaccess.thecvf.com/content_CVPR_2019/papers/Fu_Dual_Attention_Network_for_Scene_Segmentation_CVPR_2019_paper.pdf"}},[e._v("Link")])]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://drive.google.com/file/d/1V4LyFlj5g6SxekJYcY_-Xh4W4cx2ADvT/view?usp=sharing"}},[e._v("Link")])])]),a("tr",[a("td",{attrs:{width:"10%",align:"center"}},[e._v("VQA")]),a("td",{attrs:{width:"70%",align:"center"}},[e._v("Multi-modal Factorized Bilinear Pooling with Co-Attention Learning for VQA")]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://openaccess.thecvf.com/content_ICCV_2017/papers/Yu_Multi-Modal_Factorized_Bilinear_ICCV_2017_paper.pdf"}},[e._v("Link")])]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://drive.google.com/file/d/1qJuRmi96Pi9r_dUHamMewmP82U7n9JC-/view?usp=sharing"}},[e._v("Link")])])]),a("tr",[a("td",{attrs:{width:"10%",align:"center"}},[e._v("Talking Face Generation")]),a("td",{attrs:{width:"70%",align:"center"}},[e._v("Talking Face Generation by Adversarially Disentangled Audio-Visual Representation")]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:"https://ojs.aaai.org/index.php/AAAI/article/view/4967"}},[e._v("Link")])]),a("td",{attrs:{width:"10%",align:"center"}},[a("a",{attrs:{href:""}},[e._v("Link")])])])])])}],r={components:{}},s=r,o=a("2877"),l=Object(o["a"])(s,i,n,!1,null,null,null);t["default"]=l.exports},c6fb:function(e,t,a){"use strict";a.r(t);var i=function(){var e=this,t=e.$createElement;e._self._c;return e._m(0)},n=[function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("section",[a("h2",[e._v("Introduction")]),a("p",[e._v("情感分析是自然语言处理领域的热点研究问题，主要思想是通过对自然语句进行一系列处理，获取该语句的情感倾向，挖掘深层信息。在互联网已经广泛普及的现代社会，每天出现在互联网上的评论信息数以亿计，如果能对如此庞大的信息数据集加以有效利用，将产生很高的社会效益。情感分析通常在词的维度上进行。首先需要对相关文本进行分词处理，然后根据已有的情感字典，对分好的词语经过相关模型的一系列处理，得到一个有关该文本的情感分析结果。目前经常用来进行情感分析的模型有Naive Bayes、SVM、Logistic Regression等。")]),a("h2",[e._v("Project Task")]),a("ul",[a("li",[a("p",[e._v("针对网络上已有的语料信息，使用经典结构模型，对语料进行情感分析，使得训练后的模型能够有效对新输入的语料进行情感的判别。")])]),a("li",[a("p",[e._v("尝试对经典模型进行优化，使结果更准确。鼓励使用deep learning相关结构来考虑。")])])]),a("h2",[e._v("Schedule")]),a("p",[e._v("第2周：中秋节放假")]),a("p",[e._v("第3-4周：搭建所需环境python+TensorFlow，推荐使用pyCharm作为集成开发环境，了解python和TensorFlow框架的使用。")]),a("p",[e._v("第5周：国庆节放假")]),a("p",[e._v("第6-8周：尝试搭建经典模型，选定一个数据集进行训练，并给出训练后的测试结果。")]),a("p",[e._v("第9-10周：对多个经典模型(至少3个)进行评估，给出评估指标，作为今后优化的依据。")]),a("p",[e._v("第11-13周：尝试对模型进行优化，或者设计自己的模型（推荐deep learning相关知识）")]),a("p",[e._v("第14-15周：结果汇总，制作报告，撰写论文")]),a("p",[e._v("第16-17周：presentation")]),a("h2",[e._v("Resources")]),a("ul",[a("li",[a("p",[e._v("简易版教程与demo：https://github.com/aditya-xq/Text-Emotion-Detection-Using-NLP")])]),a("li",[a("p",[e._v("数据集：")]),a("ul",[a("li",[e._v("微博情感分析测评数据：https://pan.baidu.com/s/1psjysSXpKOEb1ciem7DsRw 密码：7hb4")]),a("li",[e._v("中文对话情绪语料：https://github.com/xxxspy/Chinese_conversation_sentiment")])])]),a("li",[a("p",[e._v("相关文献：")]),a("ul",[a("li",[e._v("文本情感分类：https://pdfs.semanticscholar.org/c804/78e361ed8f5fd5400fdbd4f6a6f37a2e4b57.pdf")]),a("li",[e._v("deep learning与情感分析：https://link.springer.com/content/pdf/10.1007%2Fs13042-018-0799-4.pdf")])])]),a("li",[a("p",[e._v("安装教程：")]),a("ul",[a("li",[e._v("Anaconda3(python环境管理器)安装教程：https://www.jianshu.com/p/026a2c43b081")]),a("li",[e._v("pyCharm安装及配置教程：https://www.runoob.com/w3cnote/pycharm-windows-install.html")])])])])])}],r={components:{}},s=r,o=a("2877"),l=Object(o["a"])(s,i,n,!1,null,null,null);t["default"]=l.exports},c884:function(e,t,a){var i={"./AR.md":"d7b9","./ChinaVis2021.md":"a5a9","./Nature.md":"7f89","./PaperReading.md":"b732","./VAST2012.md":"5b46","./VAST2014.md":"d308","./VAST2015.md":"d6be","./VAST2016.md":"38a8","./VAST2018.md":"6f20","./VAST2018_2.md":"a2b5","./VAST2019.md":"46c4","./VR.md":"5fbb","./business.md":"0f23","./openpose.md":"5072","./sentiment.md":"c6fb"};function n(e){var t=r(e);return a(t)}function r(e){if(!a.o(i,e)){var t=new Error("Cannot find module '"+e+"'");throw t.code="MODULE_NOT_FOUND",t}return i[e]}n.keys=function(){return Object.keys(i)},n.resolve=r,e.exports=n,n.id="c884"},d308:function(e,t,a){"use strict";a.r(t);var i=function(){var e=this,t=e.$createElement;e._self._c;return e._m(0)},n=[function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("section",[a("h2",[e._v("Challenge Descriptions")]),a("p",[e._v("In the roughly twenty years that Tethys-based GAStech has been operating a natural gas production site in the island country of Kronos, it has produced remarkable profits and developed strong relationships with the government of Kronos. However, GAStech has not been as successful in demonstrating environmental stewardship.")]),a("p",[e._v("In January, 2014, the leaders of GAStech are celebrating their new-found fortune as a result of the initial public offering of their very successful company. In the midst of this celebration, several employees of GAStech go missing. An organization known as the Protectors of Kronos (POK) is suspected in the disappearance, but things may not be what they seem.")]),a("p",[e._v("Many of the Abila, Kronos-based employees of GAStech have company cars which are approved for both personal and business use. Those who do not have company cars have the ability to check out company trucks for business use, but these trucks cannot be used for personal business.")]),a("p",[e._v("Employees with company cars are happy to have these vehicles, because the company cars are generally much higher quality than the cars they would be able to afford otherwise. However, GAStech does not trust their employees. Without the employees’ knowledge, GAStech has installed geospatial tracking software in the company vehicles. The vehicles are tracked periodically as long as they are moving.")]),a("p",[e._v("This vehicle tracking data has been made available to law enforcement to support their investigation. Unfortunately, data is not available for the day the GAStech employees went missing. Data is only available for the two weeks prior to the disappearance.")]),a("p",[e._v("In addition to the vehicle data, law enforcement has been given access to the personal and business credit and debit card transactions for the local GAStech employees for the two weeks preceding the kidnapping. Many of the GAStech employees also use loyalty cards to gain discounts or extra benefits at the businesses they patronize, and law enforcement has been given access to two weeks of this loyalty card data as well.")]),a("p",[e._v("As a visual analytics expert assisting law enforcement, your mission is to make sense of this data to identify suspicious patterns of behavior and to prioritize which of these may be related to the missing staff members. You must cope with uncertainties that result from missing, conflicting, and imperfect data to make recommendations for further investigation.")]),a("p",[e._v("Use visual analytics to analyze the available data and develop responses to the questions below. In addition, prepare a video that shows how you used visual analytics to solve this challenge. Submission instructions are available here. Entry forms are available for download below.")]),a("h2",[e._v("Project Task")]),a("ul",[a("li",[a("p",[e._v("Describe common daily routines for GAStech employees. What does a day in the life of a typical GAStech employee look like?")])]),a("li",[a("p",[e._v("Identify up to twelve unusual events or patterns that you see in the data. If you identify more than twelve patterns during your analysis, focus your answer on the patterns you consider to be most important for further investigation to help find the missing staff members. For each pattern or event you identify, describe")]),a("ul",[a("li",[a("p",[e._v("What is the pattern or event you observe?")])]),a("li",[a("p",[e._v("Who is involved?")])]),a("li",[a("p",[e._v("What locations are involved?")])]),a("li",[a("p",[e._v("When does the pattern or event take place?")])]),a("li",[a("p",[e._v("Why is this pattern or event significant?")])]),a("li",[a("p",[e._v("What is your level of confidence about this pattern or event? Why?")])])])]),a("li",[a("p",[e._v("Like most datasets, the data you were provided is imperfect, with possible issues such as missing data, conflicting data, data of varying resolutions, outliers, or other kinds of confusing data. Considering MC2 data is primarily spatiotemporal, describe how you identified and addressed the uncertainties and conflicts inherent in this data to reach your conclusions in questions MC2.1 and MC2.2.")])])]),a("h2",[e._v("Schedule")]),a("ul",[a("li",[a("p",[e._v("Fetch Dataset, analyse tasks, and make a plan to solve it (2-3 week)")])]),a("li",[a("p",[e._v("Process data, implement analysis framework (4-7 week)")])]),a("li",[a("p",[e._v("Mid-term inspection (8 week)")])]),a("li",[a("p",[e._v("Implement analysis system (9-12 week)")])]),a("li",[a("p",[e._v("Inter-team evaluation and modification (13-15 week)")])]),a("li",[a("p",[e._v("Final inspection (16-17 week)")])])]),a("h2",[e._v("Resources")]),a("ul",[a("li",[e._v("http://www.vacommunity.org/VAST+Challenge+2014%3A+Mini-Challenge+2")])])])}],r={components:{}},s=r,o=a("2877"),l=Object(o["a"])(s,i,n,!1,null,null,null);t["default"]=l.exports},d6be:function(e,t,a){"use strict";a.r(t);var i=function(){var e=this,t=e.$createElement;e._self._c;return e._m(0)},n=[function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("section",[a("h2",[e._v("Challenge Descriptions")]),a("p",[e._v("DinoFun World is a typical modest-sized amusement park, sitting on about 215 hectares and hosting thousands of visitors each day. It has a small town feel, but it is well known for its exciting rides and events.")]),a("p",[e._v("One event last year was a weekend tribute to Scott Jones, internationally renowned football (“soccer,” in US terminology) star. Scott Jones is from a town nearby DinoFun World. He was a classic hometown hero, with thousands of fans who cheered his success as if he were a beloved family member. To celebrate his years of stardom in international play, DinoFun World declared “Scott Jones Weekend”, where Scott was scheduled to appear in two stage shows each on Friday, Saturday, and Sunday to talk about his life and career. In addition, a show of memorabilia related to his illustrious career would be displayed in the park’s Pavilion.")]),a("p",[e._v("However, the event did not go as planned. Scott’s weekend was marred by crime and mayhem perpetrated by a poor, misguided and disgruntled figure from Scott’s past.")]),a("p",[e._v("While the crimes were rapidly solved, park officials and law enforcement figures are interested in understanding just what happened during that weekend to better prepare themselves for future events. They are interested in understanding how people move and communicate in the park, as well as how patterns changes and evolve over time, and what can be understood about motivations for changing patterns.")]),a("p",[e._v("You have access to movement tracking information for all of the paying park visitors over the three days of the Scott Jones celebration. This data contains many patterns that are useful for planning park operations. On this particular weekend a crime occurred and the data likely contains information pertinent to that crime.")]),a("p",[e._v("Use visual analytics to analyze the available data and develop responses to the questions below. In addition, prepare a video that shows how you used visual analytics to solve this challenge. We encourage novel visual representations and analytic approaches!")]),a("h2",[e._v("Project Task")]),a("ul",[a("li",[a("p",[e._v("Characterize the attendance at the park on this weekend. Describe up to twelve different types of groups at the park on this weekend.")]),a("ul",[a("li",[e._v("How big is the group type?")]),a("li",[e._v("Where does this type of group like to go in the park?")]),a("li",[e._v("How common is this type of group?")]),a("li",[e._v("What are your other observations about this type of group?")]),a("li",[e._v("What can you infer about the group?")]),a("li",[e._v("If you were to make one improvement to the park to better meet this group’s needs, what would it be?")])])]),a("li",[a("p",[e._v("Are there notable differences in the patterns of activity on in the park across the three days? Please describe the notable difference you see.")])]),a("li",[a("p",[e._v("What anomalies or unusual patterns do you see? Describe no more than 10 anomalies, and prioritize those unusual patterns that you think are most likely to be relevant to the crime.")])])]),a("h2",[e._v("Schedule")]),a("ul",[a("li",[a("p",[e._v("Fetch Dataset, analyse tasks, and make a plan to solve it (2-3 week)")])]),a("li",[a("p",[e._v("Process data, implement analysis framework (4-7 week)")])]),a("li",[a("p",[e._v("Mid-term inspection (8 week)")])]),a("li",[a("p",[e._v("Implement analysis system (9-12 week)")])]),a("li",[a("p",[e._v("Inter-team evaluation and modification (13-15 week)")])]),a("li",[a("p",[e._v("Final inspection (16-17 week)")])])]),a("h2",[e._v("Resources")]),a("ul",[a("li",[e._v("http://www.vacommunity.org/2015+VAST+Challenge%3A+MC1")])])])}],r={components:{}},s=r,o=a("2877"),l=Object(o["a"])(s,i,n,!1,null,null,null);t["default"]=l.exports},d7b9:function(e,t,a){"use strict";a.r(t);var i=function(){var e=this,t=e.$createElement;e._self._c;return e._m(0)},n=[function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("section",[a("h2",[e._v("Introduction")]),a("p",[e._v("Augmented reality (AR) is an interactive experience of a real-world environment where the objects that reside in the real-world are enhanced by computer-generated perceptual information, sometimes across multiple sensory modalities. Creating immersive visualizations remains challenging, and often require complex low-level programming and tedious manual encoding of data attributes to geometric and visual properties.")]),a("p",[e._v("Advantages of AR for visual exploration:")]),a("ol",[a("li",[a("p",[e._v("More data visualization possibilities: many more dimensions than the traditional placement coordinates (X,Y, and Z) become available. It can even be classified according to direction or magnitude of a vector.")])]),a("li",[a("p",[e._v("Intuitive approach: the way AR will present data is the way we interact with the world at large.")])]),a("li",[a("p",[e._v("Multiple users: when data is presented in AR, multiple users can inhabit the environment at the same time.")])]),a("li",[a("p",[e._v("Eliminating distractions: with a user tapped into data presented in AR, their visual and to some extent aural senses are completely governed by the virtual environment.")])])]),a("p",[e._v("This project will be built on DXR toolkit. DXR is help developers efficiently specify visualization designs using a concise declarative visualization grammar inspired by Vega-Lite. A GUI is provided for easy and quick edits and previews of visualization designs in-situ. Reusable templates and customizable graphical marks are also provided to enable unique and engaging visualizations.")]),a("p",[e._v("A typical DXR toolkit system scenario is as below: (1) the designer describes the visualization design in a concise specification (vis-specs) using DXR’s high-level visualization grammar. (2) DXR then infers missing visualization parameters with sensible defaults. (3) Based on this complete specification, DXR then programmatically constructs the 3D visualization that the designer can place in a real or virtual immersive scene.")]),a("h2",[e._v("Project Task")]),a("ul",[a("li",[a("p",[e._v("Run demos and examples in an AR environment.")])]),a("li",[a("p",[e._v("Summarize the shortcomings of visualizations in this toolkit and consider methods to improve them.")])])]),a("h2",[e._v("Schedule")]),a("ul",[a("li",[a("p",[e._v("Reproduce the paper result (3-5 week)")])]),a("li",[a("p",[e._v("Find some patterns in a dataset (6-7 week)")])]),a("li",[a("p",[e._v("Conclude the shortcoming of the paper in AR scenarios (6-7 week)")])]),a("li",[a("p",[e._v("Mid-term inspection (8 week)")])]),a("li",[a("p",[e._v("Find a way to optimize the toolkit (9-15 week)")])]),a("li",[a("p",[e._v("Final inspection (16-17 week)")])])]),a("h2",[e._v("Implementation tools")]),a("ul",[a("li",[a("p",[e._v("DXR")])]),a("li",[a("p",[e._v("HoloLens")])]),a("li",[a("p",[e._v("HoloToolkit (included in DXR source)")])]),a("li",[a("p",[e._v("Unity Editor")])])]),a("h2",[e._v("Resources")]),a("ul",[a("li",[a("p",[e._v("Paper: https://vcg.seas.harvard.edu/publications/dxr-a-toolkit-for-building-immersive-data-visualizations")])]),a("li",[a("p",[e._v("Source Code: https://github.com/ronellsicat/DxR")])]),a("li",[a("p",[e._v("Datasets:")]),a("ul",[a("li",[e._v("DXR Data (included in source): https://github.com/ronellsicat/DxR/tree/master/Assets/StreamingAssets/DxRData")]),a("li",[e._v("IATK Data: https://github.com/MaximeCordeil/IATK/tree/master/Assets/Datasets")])])])])])}],r={components:{}},s=r,o=a("2877"),l=Object(o["a"])(s,i,n,!1,null,null,null);t["default"]=l.exports},edd4:function(e){e.exports=JSON.parse('{"bigdataTitle":"Neural Network and Deep Learning","homePage":"Home","projectList":"Projects","resultList":"Resource","courseResources":"Resources","courseEntries":"Classroom","courseEntriesHint":"(Only for students in class)","location":"Shandong University","courseIntro":"Introduction","courseIntroContent":"As a fundamental course in the artificial intelligence course system, neural network and deep learning have been incorporated into the training program of the universities in China that offer artificial intelligence majors. The course is designed for our undergraduate students majoring in artificial intelligence and big data, aiming to provide students with a solid foundation in mathematics and theoretical knowledge of artificial intelligence, and to cultivate students\' abilities in experimental development and scientific research.","courseDesc":"Course Description","courseDescContent":"This neural network and deep learning course provides a broad introduction to the foundations of deep learning, practical skills like how to build neural networks and how to lead successful machine learning projects, and basic models like Convolutional networks, RNNs, LSTM, and more. This course will also provide case studies from healthcare, autonomous driving, and natural language processing, etc. The students need to master the theory and practice the ideas by applying methods in real-world data.","aims":"Teaching Objectives","aims1":"The course is task-driven and teaches the basic concepts and methods of program design through examples to cultivate students\' good programming habits and understand the engineering specifications of system development.","aims2":"Emphasis on hands-on practice computer programming as the starting point, focusing on the design of ideas, that is, problem analysis, model construction, algorithm design and program implementation.","aims3":"Cultivate the ability to deal with practical problems, to achieve data sampling, storage and management according to the specific needs of the problem, and give the appropriate algorithm design.","aims4":"Be able to select appropriate design tools and apply appropriate relevant technical means to improve the ability and efficiency of solving complex engineering problems.","request":"Goals","requestContent":"Through the study of this course, the following abilities will be cultivated:","requestContent1":"Engineering knowledge. Train students to establish basic concepts of neural networks and deep learning, and understand the architecture of mainstream neural networks.","requestContent2":"Problem analysis. Help students to develop the thinking of problem analysis and problem solving, and train students\' ability to abstract analyze and identify a system or process, select or build a model abstract expression, and carry out reasoning, solution and verification.","requestContent3":"Design/develop solutions. Train students to master the use of neural networks, effectively design, train, and apply deep learning techniques, and equip students with the ability of deep learning algorithm design, analysis and optimization.","reference":"Textbooks and Reference Materials","authors":"Authors","pub":"Press","pubYear":"Publication year","ptPress":"The People\'s Posts and Telecommunications Press","phei":"Publishing House of Electronics Industry","noResult":"No demo found","fetching":"Fetching file list...","fileName":"File Name","url":"URL","teacherAndTA":"Teacher and TAs","LOT":"List of Topics Covered","week":"Week","date":"Date","topic":"Topic","grading":"Course Grading","exp":"Experiment","paperreading":"Paper Reading Presentation","finalexam":"Final Exam","reslink":"Link"}')}});
//# sourceMappingURL=app.b7275cfd.js.map